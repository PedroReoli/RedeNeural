# ğŸ§  ClassificaÃ§Ã£o de Pontos com Rede Neural Simples

Este projeto demonstra o uso de uma **rede neural de camada Ãºnica** para resolver um problema de classificaÃ§Ã£o simples: **identificar se um ponto bidimensional estÃ¡ dentro ou fora de um cÃ­rculo**. A rede Ã© treinada usando duas funÃ§Ãµes de ativaÃ§Ã£o diferentes: **Sigmoid** e **ReLU**, permitindo a comparaÃ§Ã£o de desempenho entre elas.

---

## ğŸ“Œ VisÃ£o Geral

O cÃ³digo realiza as seguintes etapas:

1. **GeraÃ§Ã£o de dados**: cria 500 pontos aleatÃ³rios no espaÃ§o 2D.
2. **ClassificaÃ§Ã£o dos pontos**: define como "1" os pontos dentro de um cÃ­rculo de raio 0.5.
3. **VisualizaÃ§Ã£o**: exibe os pontos coloridos por classe.
4. **Treinamento da rede**: comparamos o uso das funÃ§Ãµes de ativaÃ§Ã£o **Sigmoid** e **ReLU**.
5. **Plotagem do erro**: exibe a evoluÃ§Ã£o do erro durante o treinamento.
6. **Testes**: aplica a rede a algumas entradas e compara a saÃ­da obtida com a esperada.

---

## ğŸ“ˆ VisualizaÃ§Ã£o do Dataset

O dataset consiste em pontos gerados aleatoriamente no intervalo [-1, 1] para as duas coordenadas. A classe de cada ponto Ã© definida com base na sua distÃ¢ncia Ã  origem (0,0), formando um cÃ­rculo:

- **Classe 1**: ponto dentro do cÃ­rculo (distÃ¢ncia < 0.5)  
- **Classe 0**: ponto fora do cÃ­rculo

O grÃ¡fico inicial mostra a distribuiÃ§Ã£o dos pontos e suas respectivas classes.

---

## âš™ï¸ FunÃ§Ãµes de AtivaÃ§Ã£o

Duas funÃ§Ãµes clÃ¡ssicas de ativaÃ§Ã£o sÃ£o implementadas:

### Sigmoid
```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

### Derivada da Sigmoid
```python
def sigmoid_derivative(x):
    return sigmoid(x) * (1 - sigmoid(x))
```

### ReLU
```python
def relu(x):
    return np.maximum(0, x)
```

### Derivada da ReLU
```python
def relu_derivative(x):
    return np.where(x > 0, 1, 0)
```

As derivadas sÃ£o essenciais para o processo de **backpropagation**, pois determinam como os pesos serÃ£o ajustados.

---

## ğŸ§ª Treinamento da Rede Neural

A rede possui:

- **2 entradas** (as coordenadas X e Y do ponto)
- **1 neurÃ´nio na saÃ­da**
- **Sem camadas ocultas**
- **Taxa de aprendizado**: `0.5`
- **Ã‰pocas**: `200.000`

Durante o treinamento:

1. Realiza-se o **forward pass** com a funÃ§Ã£o de ativaÃ§Ã£o escolhida.
2. Calcula-se o **erro quadrÃ¡tico mÃ©dio** entre a saÃ­da esperada e a obtida.
3. Aplica-se o **backpropagation**, ajustando pesos e viÃ©s com base na derivada da funÃ§Ã£o de ativaÃ§Ã£o.

A cada 5.000 Ã©pocas, o erro atual Ã© exibido no console.

---

## ğŸ“Š GrÃ¡fico de Erro

ApÃ³s o treinamento com as duas funÃ§Ãµes de ativaÃ§Ã£o, o cÃ³digo plota um grÃ¡fico com a evoluÃ§Ã£o do erro em cada Ã©poca:

- ğŸ”µ **Sigmoid**: linha azul sÃ³lida  
- ğŸ”´ **ReLU**: linha vermelha tracejada

Este grÃ¡fico facilita a comparaÃ§Ã£o da taxa de convergÃªncia entre as duas abordagens.

---

## ğŸ§ª Testes Finais

Ao final do treinamento, o modelo Ã© testado com os **10 primeiros pontos do dataset**. Para cada entrada, sÃ£o exibidos:

- As coordenadas do ponto (entrada)
- A classe esperada (rÃ³tulo real)
- A saÃ­da gerada pela rede

O teste Ã© feito separadamente para os modelos treinados com Sigmoid e ReLU.

---

## ğŸ“ Requisitos

Para executar este projeto, vocÃª precisarÃ¡ do Python 3 e das seguintes bibliotecas:

```bash
pip install numpy matplotlib
```

---

## ğŸ“ ExecuÃ§Ã£o

Salve o cÃ³digo-fonte em um arquivo chamado `neural_circle_classification.py` e execute com:

```bash
python neural_circle_classification.py
```

---

## ğŸ“ Objetivo Educacional

Este projeto foi desenvolvido com fins educacionais, com o objetivo de:

- Demonstrar o funcionamento bÃ¡sico de uma rede neural simples
- Comparar funÃ§Ãµes de ativaÃ§Ã£o no aprendizado de mÃ¡quina
- Promover entendimento de conceitos como **forward pass**, **backpropagation** e **gradiente descendente**

---

## ğŸ‘¥ Autores

Este projeto foi desenvolvido por:

- **Pedro Lucas Reis de Oliveira Sousa**  
- **Pedro Amando Gandos Citelli**
